---
title: NTUML - Deep Learning - Batch Normalization
---

<!-- more -->

<iframe src="https://www.youtube.com/embed/BABPWOkSbLE" allowfullscreen></iframe>

Error Surface 崎岖 $\rightarrow$**把山铲平**的想法

![image-20210422182339562](https://lllthhhh-aliyun-oss.oss-cn-beijing.aliyuncs.com/img/20210422182339.png)

不同方向斜率不同的情况, 进行 **Normalization**

![image-20210422182514002](https://lllthhhh-aliyun-oss.oss-cn-beijing.aliyuncs.com/img/20210422182514.png)

均值是0, 方差是1

![image-20210422182627964](https://lllthhhh-aliyun-oss.oss-cn-beijing.aliyuncs.com/img/20210422182628.png)

对于后面的层, 也有输入, 继续做 **normalization**. 在激活函数前后做差异不大

对于sigmoid, 之前好一点, 因为在0附近斜率大

![image-20210422182915655](https://lllthhhh-aliyun-oss.oss-cn-beijing.aliyuncs.com/img/20210422182915.png)



数据集很大, 全部做Normalization不现实, 选择一个batch

适用于batch大的情况

后期可以加入另外两个参数$\beta, \gamma$

去学习分布(?)

![image-20210422183500356](https://lllthhhh-aliyun-oss.oss-cn-beijing.aliyuncs.com/img/20210422183500.png)

![image-20210422193924920](https://lllthhhh-aliyun-oss.oss-cn-beijing.aliyuncs.com/img/20210422193925.png)

Testing的时候使用均值进行替代, 不使用batch