---
title: NTUML - Deep Learning - Tips for Training: Batch and Momentum
---

<!-- more -->

<iframe src="https://www.youtube.com/embed/zzbr1h9sF54" allowfullscreen></iframe>

## Batch

![image-20210422163801592](https://lllthhhh-aliyun-oss.oss-cn-beijing.aliyuncs.com/img/20210422163801.png)

![image-20210422163830305](https://lllthhhh-aliyun-oss.oss-cn-beijing.aliyuncs.com/img/20210422163830.png)

由于GPU并行运算, cooldown并不成立

![image-20210422164306128](https://lllthhhh-aliyun-oss.oss-cn-beijing.aliyuncs.com/img/20210422164306.png)

但是noisy的可能会帮助training

![image-20210422164443159](https://lllthhhh-aliyun-oss.oss-cn-beijing.aliyuncs.com/img/20210422164443.png)

![image-20210422164508787](https://lllthhhh-aliyun-oss.oss-cn-beijing.aliyuncs.com/img/20210422164508.png)

training差不多的情况下, 在testing上 小的batch结果也好, 大的出现了 **overfitting**

![image-20210422164918456](https://lllthhhh-aliyun-oss.oss-cn-beijing.aliyuncs.com/img/20210422164918.png)

直觉解释: 小的batch可能会跳出峡谷 到达一个flat的minima

![image-20210422165054100](https://lllthhhh-aliyun-oss.oss-cn-beijing.aliyuncs.com/img/20210422165054.png)

```
Large Batch Optimization for Deep Learning: Training BERT in 76 minutes (https://arxiv.org/abs/1904.00962)
Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes (https://arxiv.org/abs/1711.04325)
Stochastic Weight Averaging in Parallel: Large-Batch Training That Generalizes Well (https://arxiv.org/abs/2001.02312)
Large Batch Training of Convolutional Networks (https://arxiv.org/abs/1708.03888)
Accurate, large minibatch sgd: Training imagenet in 1 hour (https://arxiv.org/abs/1706.02677)
```

## Momentum

![image-20210422165324910](https://lllthhhh-aliyun-oss.oss-cn-beijing.aliyuncs.com/img/20210422165324.png)

初始的梯度下降

![image-20210422165417119](https://lllthhhh-aliyun-oss.oss-cn-beijing.aliyuncs.com/img/20210422165417.png)

加动量

![image-20210422165501593](https://lllthhhh-aliyun-oss.oss-cn-beijing.aliyuncs.com/img/20210422165501.png)